{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list that contains the URL for each page.\n",
    "pages = []\n",
    "\n",
    "# There are 130 pages in the entire set.\n",
    "for page_number in range(1, 130):\n",
    "    url_start = 'https://www.centralcharts.com/en/price-list-ranking/'\n",
    "    url_end = 'ALL/asc/ts_19-us-nasdaq-stocks--qc_1-alphabetical-order?p='\n",
    "    url = url_start + url_end + str(page_number)\n",
    "    pages.append(url)\n",
    "\n",
    "# Create a list containing the <th> tag contents.\n",
    "webpage = requests.get(pages[0])\n",
    "soup = bs(webpage.text, 'html.parser')\n",
    "stock_table = soup.find('table', class_='tabMini tabQuotes')\n",
    "th_tag_list = stock_table.find_all('th')\n",
    "\n",
    "# Take the <th> list and remove the extra attributes to\n",
    "# get just the text portion of each tag. Use this text for the \n",
    "# column header labels in the dataframe.\n",
    "headers = []\n",
    "for each_tag in th_tag_list:\n",
    "    title = each_tag.text\n",
    "    headers.append(title)\n",
    "\n",
    "headers[0] = 'Name'\n",
    "\n",
    "new_headers = []\n",
    "for header in headers:\n",
    "    if header not in ('Cap.', 'Issued Cap.', ''):\n",
    "        new_headers.append(header)\n",
    "headers = new_headers\n",
    "stock_df = pd.DataFrame(columns = headers)\n",
    "\n",
    "\n",
    "# Cycle through each page. \n",
    "for page in pages:\n",
    "    webpage = requests.get(page)\n",
    "    soup = bs(webpage.text, 'html.parser')\n",
    "\n",
    "    # Check to see if the page contains a table. If it does,\n",
    "    # create a list of <tr> tags. If not, go to the next page.\n",
    "    if soup.find('table'):\n",
    "        stock_table = soup.find('table', class_='tabMini tabQuotes')\n",
    "        tr_tag_list = stock_table.find_all('tr')\n",
    "\n",
    "        # Cycle through the <tr> list. For each\n",
    "        # row, find the <td> tags within the row. Then\n",
    "        # obtain the text within each <td> tag. Lastly, \n",
    "        # place the text in the last row of the dataframe.\n",
    "        for each_tr_tag in tr_tag_list[1:]:\n",
    "            td_tag_list = each_tr_tag.find_all('td')\n",
    "\n",
    "            row_values = []\n",
    "            for each_td_tag in td_tag_list[0:7]:\n",
    "                new_value = each_td_tag.text.strip()\n",
    "                row_values.append(new_value)\n",
    "\n",
    "            stock_df.loc[len(stock_df)] = row_values\n",
    "\n",
    "\n",
    "# Change the data types & sort by trading volume\n",
    "stock_df[['Name', 'Current price', 'Change(%)', 'Open','High', 'Low']] = \\\n",
    "    stock_df[['Name', 'Current price', 'Change(%)', 'Open', 'High', 'Low']] \\\n",
    "    .astype(str)\n",
    "\n",
    "stock_df.replace({'Current price': {',':''},\n",
    "                  'Change(%)': {',':'', '%':''},\n",
    "                  'Open': {',':''},\n",
    "                  'High': {',':''},\n",
    "                  'Low': {',':''},\n",
    "                  'Volume': {',':''}\n",
    "}, regex=True, inplace=True)\n",
    "\n",
    "\n",
    "stock_df['timestamp'] = datetime.now()\n",
    "\n",
    "# stock_df.to_csv()\n",
    "nasdaq_df = stock_df.rename(columns={'Current price': 'Current_price', 'Change(%)': 'Change'})\n",
    "# if not os.path.exists(data_path):\n",
    "#     os.makedirs(data_path)\n",
    "\n",
    "csv_time = str(datetime.now())\n",
    "csv_time = csv_time.replace(\" \",\"\")\n",
    "csv_time = csv_time.replace(\":\",\"\")\n",
    "csv_time = csv_time.replace(\"-\",\"\")\n",
    "\n",
    "csv_time = csv_time[0:14]\n",
    "csv_name = 'nasdaq_'+csv_time+'.csv'\n",
    "data_path = '/lakehouse/default/Files/'+csv_name\n",
    "nasdaq_df.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_read_path = \"abfss://Hacathon@onelake.dfs.fabric.microsoft.com/Stock_data_lake.Lakehouse/Files/\"+csv_name\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(csv_read_path)\n",
    "\n",
    "df = df.withColumn(\"timestamp\",df.timestamp.cast(\"Timestamp\"))\n",
    "df = df.withColumn(\"Current_price\",df.Current_price.cast(\"Double\"))\n",
    "df = df.withColumn(\"Change\",df.Change.cast(\"Double\"))\n",
    "df = df.withColumn(\"Open\",df.Open.cast(\"Double\"))\n",
    "df = df.withColumn(\"High\",df.High.cast(\"Double\"))\n",
    "df = df.withColumn(\"Low\",df.Low.cast(\"Double\"))\n",
    "df = df.withColumn(\"Volume\",df.Volume.cast(\"Integer\"))\n",
    "\n",
    "delta_file_path = \"abfss://Hacathon@onelake.dfs.fabric.microsoft.com/Stock_data_lake.Lakehouse/Tables/nasdaq\"\n",
    "df.write.mode(\"append\").format(\"delta\").save(delta_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list that contains the URL for each page.\n",
    "pages = []\n",
    "\n",
    "# There are 94 pages in the entire set.\n",
    "for page_number in range(1, 94):\n",
    "    url_start = 'https://www.centralcharts.com/en/price-list-ranking/'\n",
    "    url_end = 'ALL/asc/ts_29-us-nyse-stocks--qc_1-alphabetical-order?p='\n",
    "    url = url_start + url_end + str(page_number)\n",
    "    pages.append(url)\n",
    "\n",
    "\n",
    "# Create a list containing the <th> tag contents.\n",
    "webpage = requests.get(pages[0])\n",
    "soup = bs(webpage.text, 'html.parser')\n",
    "stock_table = soup.find('table', class_='tabMini tabQuotes')\n",
    "th_tag_list = stock_table.find_all('th')\n",
    "\n",
    "# Take the <th> list and remove the extra attributes to\n",
    "# get just the text portion of each tag. Use this text for the \n",
    "# column header labels in the dataframe.\n",
    "headers = []\n",
    "for each_tag in th_tag_list:\n",
    "    title = each_tag.text\n",
    "    headers.append(title)\n",
    "\n",
    "headers[0] = 'Name'\n",
    "\n",
    "new_headers = []\n",
    "for header in headers:\n",
    "    if header not in ('Cap.', 'Issued Cap.', ''):\n",
    "        new_headers.append(header)\n",
    "headers = new_headers\n",
    "stock_df = pd.DataFrame(columns = headers)\n",
    "\n",
    "\n",
    "# Cycle through each page. \n",
    "for page in pages:\n",
    "    webpage = requests.get(page)\n",
    "    soup = bs(webpage.text, 'html.parser')\n",
    "\n",
    "    # Check to see if the page contains a table. If it does,\n",
    "    # create a list of <tr> tags. If not, go to the next page.\n",
    "    if soup.find('table'):\n",
    "        stock_table = soup.find('table', class_='tabMini tabQuotes')\n",
    "        tr_tag_list = stock_table.find_all('tr')\n",
    "\n",
    "        # Cycle through the <tr> list. For each\n",
    "        # row, find the <td> tags within the row. Then\n",
    "        # obtain the text within each <td> tag. Lastly, \n",
    "        # place the text in the last row of the dataframe.\n",
    "        for each_tr_tag in tr_tag_list[1:]:\n",
    "            td_tag_list = each_tr_tag.find_all('td')\n",
    "\n",
    "            row_values = []\n",
    "            for each_td_tag in td_tag_list[0:7]:\n",
    "                new_value = each_td_tag.text.strip()\n",
    "                row_values.append(new_value)\n",
    "\n",
    "            stock_df.loc[len(stock_df)] = row_values\n",
    "\n",
    "\n",
    "# Change the data types & sort by trading volume\n",
    "stock_df[['Name', 'Current price', 'Change(%)', 'Open','High', 'Low']] = \\\n",
    "    stock_df[['Name', 'Current price', 'Change(%)', 'Open', 'High', 'Low']] \\\n",
    "    .astype(str)\n",
    "\n",
    "stock_df.replace({'Current price': {',':''},\n",
    "                  'Change(%)': {',':'', '%':''},\n",
    "                  'Open': {',':''},\n",
    "                  'High': {',':''},\n",
    "                  'Low': {',':''},\n",
    "                  'Volume': {',':''}\n",
    "}, regex=True, inplace=True)\n",
    "\n",
    "# stock_df.replace({'Current price': {',':'', '-':'1'},\n",
    "#                   'Change(%)': {',':'', '-':'1', '%':''},\n",
    "#                   'Open': {',':'', '-':'1'},\n",
    "#                   'High': {',':'', '-':'1'},\n",
    "#                   'Low': {',':'', '-':'1'},\n",
    "#                   'Volume': {',':'', '-':'1'}\n",
    "# }, regex=True, inplace=True)\n",
    "\n",
    "# stock_df[['Current price', 'Change(%)', 'Open', 'High', 'Low', 'Volume']] = \\\n",
    "#     stock_df[['Current price', 'Change(%)', 'Open', 'High', 'Low', 'Volume']]. \\\n",
    "#     apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "stock_df['timestamp'] = datetime.now()\n",
    "\n",
    "# stock_df.to_csv()\n",
    "nyse_df = stock_df.rename(columns={'Current price': 'Current_price', 'Change(%)': 'Change'})\n",
    "# if not os.path.exists(data_path):\n",
    "#     os.makedirs(data_path)\n",
    "\n",
    "csv_time = str(datetime.now())\n",
    "csv_time = csv_time.replace(\" \",\"\")\n",
    "csv_time = csv_time.replace(\":\",\"\")\n",
    "csv_time = csv_time.replace(\"-\",\"\")\n",
    "\n",
    "csv_time = csv_time[0:14]\n",
    "csv_name = 'nyse_'+csv_time+'.csv'\n",
    "data_path = '/lakehouse/default/Files/'+csv_name\n",
    "nyse_df.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_read_path = \"abfss://Hacathon@onelake.dfs.fabric.microsoft.com/Stock_data_lake.Lakehouse/Files/\"+csv_name\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(csv_read_path)\n",
    "\n",
    "df = df.withColumn(\"timestamp\",df.timestamp.cast(\"Timestamp\"))\n",
    "df = df.withColumn(\"Current_price\",df.Current_price.cast(\"Double\"))\n",
    "df = df.withColumn(\"Change\",df.Change.cast(\"Double\"))\n",
    "df = df.withColumn(\"Open\",df.Open.cast(\"Double\"))\n",
    "df = df.withColumn(\"High\",df.High.cast(\"Double\"))\n",
    "df = df.withColumn(\"Low\",df.Low.cast(\"Double\"))\n",
    "df = df.withColumn(\"Volume\",df.Volume.cast(\"Integer\"))\n",
    "\n",
    "delta_file_path = \"abfss://Hacathon@onelake.dfs.fabric.microsoft.com/Stock_data_lake.Lakehouse/Tables/nyse\"\n",
    "df.write.mode(\"append\").format(\"delta\").save(delta_file_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
